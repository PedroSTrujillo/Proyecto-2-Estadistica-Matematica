---
title: "Proyecto 2"
subtitle: Sara Leiva, Pedro Pablo Sanín
output:
  pdf_document: default
  html_notebook: default
---

### Introducción

La distribución Gamma con parámetros $\alpha$ y $\beta$ se usa en las ciencias para modelar tiempos de vida de organismos o artefactos. Su densidad de probabilidad está dada por $$\frac{x^{\alpha-1} e^{-x/\beta}}{\Gamma(\alpha)\beta^\alpha}$$ y está soportada para $x > 0$. Se tiene además que $$\mu=\mathbb{E}(X) = \alpha\beta, \quad \sigma^2 = \mathrm{Var}(X)=\alpha\beta^2$$.

El objetivo de este proyecto es calcular el Estimador de Máxima Verosimilitud del parámetro $\theta = (\alpha, \beta)$ de este modelo, y verificar la validez del conjunto elíptico de confianza obtenido al estimar el parámetro con el Teorema del Límite Central para Estimadores de Máxima Verosimilitud biparamétrico.

A lo largo del proyecto, supondremos que contamos con $n$ muestras iid. $\mathcal{X} =( X_1, \dots, X_n)$ con distribución $\mathrm{Gamma}(\alpha, \beta)$ para $\alpha = \alpha_0 = 2.5$ y $\beta = \beta_0 = 4.6$.

### Cálculo numérico del Estimador de Máxima Verosimilitud para el parámetro $\theta$

#### Método de Newton-Raphson para calcular el EMV

La verosimilitud del parámetro $\theta$ para la muestra $\mathcal{X}$ está dada por la función bivariada $$L(\theta) = \prod_{i=1}^n f(X_i;\alpha,\beta)=\prod_{i=1}^n\frac{{X_i}^{\alpha-1} e^{-{X_i}/\beta}}{\Gamma(\alpha)\beta^\alpha}$$. Así, la log-verosimilitud es: $$l(\theta) = \sum_{i=1}^n \left( (\alpha-1)\ln(X_i)-\frac{X_i}{\beta} - \ln(\Gamma(\alpha))-\alpha\ln(\beta)\right)=  - n\ln(\Gamma(\alpha))-n\alpha\ln(\beta)+\sum_{i=1}^n \left( (\alpha-1)\ln(X_i)-\frac{X_i}{\beta}\right).$$

Para encontrar el valor de $\theta$ que minimiza $l(\theta)$, es decir, el estimador de máxima verosimilitud para los valores observados, debemos solucionar las ecuaciones $$\frac{\partial l(\theta)}{\partial \alpha}=0,  \qquad \frac{\partial l(\theta)}{\partial \beta}=0.$$ Vea entonces que $$\frac{\partial l(\theta)}{\partial \alpha} = -n \frac{\Gamma'(\alpha)}{\Gamma(\alpha)}-n\ln(\beta)+\sum_{i=1}^n \ln(X_i).$$ Asimismo $$\frac{\partial l(\theta)}{\partial \beta} = -\frac{n\alpha}{\beta}+\frac{1}{\beta^2}\sum_{i=1}^n X_i.$$

Para solucionar el sistema de ecuaciones, podemos usar métodos numéricos, pues la solución simbólica es difícil. En este caso, usaremos el método de Newton-Raphson bivariado. Para esto, debemos identificar un estimador inicial $\tilde{\theta}_0$ y a partir de esto construimos las aproximaciones a la raíz por la fórmula: $$\tilde{\theta}_{k+1}=\tilde{\theta}_k-[Df(\tilde{\theta}_k)]^{-1}f(\tilde{\theta}_k),$$ siendo $f:X \subseteq \mathbb{R}^2 \longrightarrow \mathbb{R}^2$ con $$f(\alpha,\beta) = (\frac{\partial l(\theta)}{\partial \alpha}(\alpha,\beta),\frac{\partial l(\theta)}{\partial \beta}(\alpha,\beta))$$

Así, si $\tilde{\theta}_k = (\tilde{\alpha}_k, \tilde{\beta}_k)$, tenemos que $$
Df(\tilde{\theta}_k)
=
\begin{pmatrix}
-n\dfrac{\Gamma''(\tilde{\alpha}_k)\Gamma(\tilde{\alpha}_k)-\Gamma'(\tilde{\alpha}_k)^2}{\Gamma(\tilde{\alpha}_k)^2}
&
-\dfrac{n}{\tilde{\beta}_k} \\[10pt]
-\dfrac{n}{\tilde{\beta}_k}
&
\dfrac{n\tilde{\alpha}_k}{\tilde{\beta}_k^2}
-2\dfrac{\sum_{i=1}^n X_i}{\tilde{\beta}_k^3}
\end{pmatrix}.
$$

Invirtiendo la matriz, obtenemos: $$
[Df(\tilde{\theta}_k)]^{-1}
=
\frac{1}{
-\,n\dfrac{\Gamma''(\tilde{\alpha}_k)\Gamma(\tilde{\alpha}_k)-\Gamma'(\tilde{\alpha}_k)^2}{\Gamma(\tilde{\alpha}_k)^2}
\!\left(
\dfrac{n\tilde{\alpha}_k}{\tilde{\beta}_k^2}
-2\dfrac{\sum_{i=1}^n X_i}{\tilde{\beta}_k^3}
\right)
-\dfrac{n^2}{\tilde{\beta}_k^2}
}
\begin{pmatrix}
\dfrac{n\tilde{\alpha}_k}{\tilde{\beta}_k^2}
-2\dfrac{\sum_{i=1}^n X_i}{\tilde{\beta}_k^3}
&
\dfrac{n}{\tilde{\beta}_k}
\\[10pt]
\dfrac{n}{\tilde{\beta}_k}
&
-\,n\dfrac{\Gamma''(\tilde{\alpha}_k)\Gamma(\tilde{\alpha}_k)-\Gamma'(\tilde{\alpha}_k)^2}{\Gamma(\tilde{\alpha}_k)^2}
\end{pmatrix}.
$$ Sabemos que, para asegurarnos de que el método de Newton-Raphson converge, debemos elegir un estimador inicial apropiado. Sabemos que podemos encontrar estimadores suficientemente buenos para $\alpha$ y $\beta$ usando los momentos de la distribución. Sabemos de las ecuaciones anteriorese que $$\alpha = \frac{\mu^2}{\sigma^2}, \qquad \beta=\frac{\sigma^2}{\mu}$$ y así, podemos definir $$\tilde{\alpha}_0 = \frac{\overline{X}^2}{s^2}, \qquad \tilde{\beta}_=\frac{s^2}{\overline{X}}$$

#### Calculando el EMV

A continuación mostramos el código R usado para calcular el estimador

```{r}
set.seed(1)

mle_gamma_nr <- function(sample, precision = 1e-6, max_iteraciones = 50){
  # Estadísticos básicos
  sample.size     <- length(sample)
  sample.mean     <- mean(sample)
  sample.variance <- var(sample)
  sample.sum      <- sum(sample)
  sample.sumlog   <- sum(log(sample))

  iteraciones <- 0L
  diferencia <- Inf

  # Gradiente de la log-verosimilitud
  likelihood_gradient <- function(theta_k){
    alpha_k <- theta_k[1]
    beta_k  <- theta_k[2]
    dalpha  <- -sample.size * digamma(alpha_k) - sample.size * log(beta_k) + sample.sumlog
    dbeta   <- (-sample.size * alpha_k / beta_k) + (sample.sum / beta_k^2)
    return(c(dalpha, dbeta))
  }

  # Inversa del Hessiano
  inverse_hessian <- function(theta_k){
    alpha_k <- theta_k[1]
    beta_k  <- theta_k[2]
    a <- -sample.size * trigamma(alpha_k)
    b <- -sample.size / beta_k
    d <- (sample.size * alpha_k / beta_k^2) - 2 * (sample.sum / beta_k^3)
    detA <- a * d - b^2
    return((1 / detA) * matrix(c(d, -b, -b, a), nrow = 2, byrow = TRUE))
  }

  # Inicialización por momentos
  theta_ant <- c((sample.mean)^2 / sample.variance, sample.variance / sample.mean)
  
  #Verificamos que los estimadores estén bien definidos y sean positivos.
  if (any(!is.finite(theta_ant)) || any(theta_ant <= 0)) {
    #theta_ant <- pmax(theta_ant, 1e-8)
  }

  repeat {
    # Paso de Newton: theta_nuevo = theta_ant - inv(H) %*% grad
    paso <- drop(inverse_hessian(theta_ant) %*% likelihood_gradient(theta_ant))
    theta_nuevo <- theta_ant - paso
    
    theta_nuevo[!is.finite(theta_nuevo) | theta_nuevo <= 0] <- 1e-8

    iteraciones <- iteraciones + 1L
    diferencia <- max(abs(theta_nuevo - theta_ant))

    if (diferencia < precision || iteraciones >= max_iteraciones) break
    theta_ant <- theta_nuevo
  }

  return(list(
    theta = theta_nuevo,
    iteraciones = iteraciones,
    convergio = (diferencia < precision)
  ))
}
```

Ahora, generaremos 1000 muestras de tamaños $n = 100, 200, 500, 1000 y 2000$, y dibujaremos para comparar los distintos estimadores obtenidos para cada $n$.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.show='hold', fig.ncol=2, out.width='49%', fig.align='center'}
set.seed(123)

# True parameters (Gamma with shape = alpha, scale = beta)
alpha_0 <- 2.5
beta_0  <- 4.6

# Sample sizes and number of replications
ns   <- c(100, 200, 500, 1000, 2000)
reps <- 1000L

# Collect MLEs for each n
resultados <- do.call(rbind, lapply(ns, function(n) {
  ests <- replicate(reps, {
    x <- rgamma(n, shape = alpha_0, scale = beta_0)
    fit <- mle_gamma_nr(x)
    fit$theta
  })
  data.frame(
    n = rep(n, reps),
    alpha_hat = ests[1, ],
    beta_hat  = ests[2, ],
    row.names = NULL
  )
}))

# Two boxplots held in a single figure by knitr
boxplot(alpha_hat ~ factor(n), data = resultados,
        xlab = "Tamaño muestral n", ylab = "EMV alpha",
        main = "Estimadores alpha por n")
abline(h = alpha_0, col = "red", lty = 2)

boxplot(beta_hat ~ factor(n), data = resultados,
        xlab = "Tamaño muestral n", ylab = "EMV beta",
        main = "Estimadores beta por n")
abline(h = beta_0, col = "red", lty = 2)
```

Podemos ver que a medida que aumenta el tamaño muestral, la varianza de los estimadores se reduce y la media tiende al parámetro real según el cual se generaron los datos.

### Conjuntos de confianza

#### TLC para el EMV y estadístico $\chi^2$

##### TLC multivariado clásico para promedios

Para comenzar, recordemos el TLC multivariado clásico para promedios.\
Si $X_1,\dots,X_n\in\mathbb{R}^d$ son i.i.d. con $\mathbb{E}(X_i)=\mu$ y $\mathrm{Cov}(X_i)=C$ (simétrica definida positiva), entonces $$
\sqrt{n}\,(\bar X-\mu)\ \xrightarrow{d}\ \mathcal{N}_d(0,\,C).
$$ Equivalentemente, $$
\sqrt{n}\,C^{-1/2}(\bar X-\mu)\ \xrightarrow{d}\ \mathcal{N}_d(0,\,I_d),
$$ y el cuadrático $$
n\,(\bar X-\mu)^\top C^{-1}(\bar X-\mu)\ \xrightarrow{d}\ \chi^2_d.
$$ Lo anterior se justifica porque, al ser $C$ simétrica definida positiva, existe una matriz ortogonal $U$ tal que $U^\top C\,U = D$, con $D=\mathrm{diag}(\lambda_1,\dots,\lambda_d)$ y $\lambda_i>0$ (autovalores de $C$). En particular, $$
C^{\pm1/2}=U\,D^{\pm1/2}U^\top,\qquad
C^{-1/2} C\, C^{-1/2}=I_d,\qquad
C^{-1/2}C^{-1/2}=C^{-1}.
$$

##### TLC del EMV

Bajo condiciones estándar de regularidad, el estimador de máxima verosimilitud (EMV) $\hat\theta_n=(\hat\alpha_n,\hat\beta_n)$ para el modelo $\mathrm{Gamma}(\alpha,\beta)$ es consistente y asintóticamente normal: $$
\sqrt{n}\,(\hat\theta_n-\theta_0)\ \xrightarrow{d}\ \mathcal{N}_2\!\big(0,\,\mathcal{I}_F(\theta_0)^{-1}\big),
\qquad
\hat\theta_n \xrightarrow{p} \theta_0,
$$ donde $\mathcal{I}_F(\theta)$ es la **información de Fisher por observación** en $\theta$. En esta formulación, $$
C=\mathcal{I}_F(\theta_0)^{-1}
$$ es la matriz de covarianzas asintótica por observación del EMV; por tanto, $$
\sqrt{n}\,\mathcal{I}_F(\theta_0)^{1/2}(\hat\theta_n-\theta_0)\ \xrightarrow{d}\ \mathcal{N}_2(0,\,I_2),\qquad
n\,(\hat\theta_n-\theta_0)^\top \mathcal{I}_F(\theta_0)(\hat\theta_n-\theta_0)\ \xrightarrow{d}\ \chi^2_2.
$$

En la práctica $\theta_0$ es desconocido. Por continuidad de $\mathcal{I}_F$ y consistencia de $\hat\theta_n$ (teorema del mapeo continuo), $$
\mathcal{I}_F(\hat\theta_n)\ \xrightarrow{p}\ \mathcal{I}_F(\theta_0),
$$ y, por Slutsky, $$
\sqrt{n}\,\mathcal{I}_F(\hat\theta_n)^{1/2}(\hat\theta_n-\theta_0)\ \xrightarrow{d}\ \mathcal{N}_2(0,\,I_2).
$$ Esto conduce al estadístico $$
Q_n(\theta_0):=n\,(\hat\theta_n-\theta_0)^\top \mathcal{I}_F(\hat\theta_n)(\hat\theta_n-\theta_0)\ \xrightarrow{d}\ \chi^2_2.
$$ Obsérvese que $\mathcal{I}_F(\hat\theta_n)$ actúa como **matriz de precisión** (inversa de la covarianza asintótica). En consecuencia, una estimación de la covarianza asintótica del EMV es $$
\widehat{\mathrm{Cov}}(\hat\theta_n)\ \approx\ \frac{1}{n}\,\mathcal{I}_F(\hat\theta_n)^{-1}.
$$

Además, para cualquier $C$ simétrica definida positiva (por ejemplo $C=\mathcal{I}_F(\theta_0)^{-1}$) existe la **raíz cuadrada principal** $C^{1/2}$, dada por la descomposición espectral $C=U D U^\top$ con $U$ ortogonal y $D$ diagonal con autovalores positivos. Esto explica la aparición de elipses al construir regiones de confianza: si definimos $$
C(\hat\theta):=\frac{1}{n}\,\mathcal{I}_F(\hat\theta)^{-1},
$$ entonces $C(\hat\theta)^{-1/2}\sqrt{n}\,(\hat\theta-\theta)=\mathcal{I}_F(\hat\theta)^{1/2}(\hat\theta-\theta)$ hace un cambio de variables que pone las dos coordenadas del error del EMV en la misma escala y sin mezclarse entre sí; así, la región de confianza es una esfera en el espacio transformado y, al volver al espacio original, una elipse.

#### Información de Fisher para $\mathrm{Gamma}(\alpha,\beta)$

Sea $X\sim \mathrm{Gamma}(\alpha,\beta)$ con densidad $$
f(x;\alpha,\beta)=\frac{x^{\alpha-1}e^{-x/\beta}}{\Gamma(\alpha)\,\beta^\alpha},\quad x>0.
$$ La log-verosimilitud por observación es $$
\ell(\alpha,\beta)=(\alpha-1)\ln X-\frac{X}{\beta}-\ln\Gamma(\alpha)-\alpha\ln\beta.
$$ El score y la Hessiana (por observación) son $$
\frac{\partial\ell}{\partial\alpha}=\ln X-\psi(\alpha)-\ln\beta,\qquad
\frac{\partial\ell}{\partial\beta}=\frac{X}{\beta^2}-\frac{\alpha}{\beta},
$$ donde $\psi(\alpha)=\dfrac{\mathrm{d}}{\mathrm{d}\alpha}\ln\Gamma(\alpha)$ es la **digamma**. Para las segundas derivadas: $$
\frac{\partial^2\ell}{\partial\alpha^2}=-h(\alpha),\qquad
\frac{\partial^2\ell}{\partial\alpha\,\partial\beta}=-\frac{1}{\beta},\qquad
\frac{\partial^2\ell}{\partial\beta^2}=\frac{\alpha}{\beta^2}-\frac{2X}{\beta^3},
$$ donde $h(\alpha)$ es la **trigamma**. Tomando esperanza y cambiando de signo, la información de Fisher por observación es $$
\mathcal{I}_F(\alpha,\beta)=
\begin{pmatrix}
h(\alpha) & \dfrac{1}{\beta}\\[6pt]
\dfrac{1}{\beta} & \dfrac{\alpha}{\beta^2}
\end{pmatrix},
$$ ya que $\mathbb{E}[X]=\alpha\beta$ implica que $$
-\mathbb{E}\!\left[\frac{\partial^2\ell}{\partial\beta^2}\right]
= -\left(\frac{\alpha}{\beta^2}-\frac{2\,\mathbb{E}[X]}{\beta^3}\right)
= -\left(\frac{\alpha}{\beta^2}-\frac{2\alpha\beta}{\beta^3}\right)
= \frac{\alpha}{\beta^2}.
$$

#### Elipse de confianza asintótica y su área

Sea $\rho=q_{0.95}(\chi^2_2)$. Un conjunto de confianza elíptico (aprox. $95\%$) para $\theta_0$ es $$
C_{0.95}=\Big\{\theta\in\mathbb{R}^2:\ n\,(\hat\theta-\theta)^\top \mathcal{I}_F(\hat\theta)\,(\hat\theta-\theta)\le \rho\Big\}.
$$ Diagonalizando $\mathcal{I}_F(\hat\theta)=U D U^\top$ con $U$ ortogonal y $D=\mathrm{diag}(\lambda_1,\lambda_2)$, los semiejes de la elipse son $$
a_i=\sqrt{\frac{\rho}{n\,\lambda_i}},\qquad i=1,2,
$$ y el área resulta $$
\mathrm{Area}(C_{0.95})=\pi a_1 a_2=\frac{\pi\,\rho}{n\,\sqrt{\det\!\big(\mathcal{I}_F(\hat\theta)\big)}}.
$$

A continuación calculamos la proporción de réplicas que cumplen la condición de contención y graficamos los EMV coloreando según satisfagan o no $n(\hat\theta-\theta_0)^\top \mathcal{I}_F(\hat\theta)(\hat\theta-\theta_0)\le \rho$. Para ello, fijamos el nivel $0.95$, obtenemos el cuantil $\rho$ de $\chi^2_2$, definimos $\mathcal{I}_F$ según lo demostrado y una función que determina si cada réplica cae en $C_{0.95}$.

```{r}
nivel <- 0.95                  # Nivel de cobertura
rho   <- qchisq(nivel, df = 2) # Cuantil de la chi-cuadrado con 2 g.l.

# Información de Fisher por observación en (alpha, beta)
I_F <- function(theta) {
  alpha <- theta[1]; beta <- theta[2]
  a <- trigamma(alpha)
  b <- 1 / beta
  d <- alpha / (beta^2)
  matrix(c(a, b, b, d), nrow = 2, byrow = TRUE)
}

# ¿La réplica cae en C_{0.95}?  (definido por la forma cuadrática <= rho)
esta_en_C <- function(theta_hat, theta0, n, rho) {
  diff <- theta_hat - theta0
  IF   <- I_F(theta_hat)
  val  <- as.numeric(n * t(diff) %*% IF %*% diff)
  val <= rho
}
```

Ahora, calculamos la **cobertura empírica** (porcentaje de réplicas dentro de $C_{0.95}$) para cada tamaño muestral $n$. Para cada réplica $j$ definimos el indicador $I_j=\mathbf{1}\{\,n(\hat\theta^{(j)}-\theta_0)^\top \mathcal{I}_F(\hat\theta^{(j)})(\hat\theta^{(j)}-\theta_0)\le \rho\,\}$.

Con $m$ réplicas independientes, $K=\sum_{j=1}^m I_j \sim \mathrm{Binomial}(m,p)$, donde $p$ es la **cobertura verdadera**, y el estimador puntual es $\hat p=K/m$.

Además, reportamos un **IC binomial del 95%** para $p$ (Clopper–Pearson vía `binom.test`), que cuantifica la **incertidumbre Monte Carlo**: si repitiéramos el estudio muchas veces, en ≈95% de los casos el intervalo $[L,U]$ contendría a $p$.

Operativamente, si $0.95\in[L,U]$ la cobertura observada es **compatible** con la nominal; si todo $[L,U]$ queda por debajo (encima) de 0.95, hay evidencia de **subcobertura** (**sobrecobertura**), respectivamente.

```{r}
calcular_probabilidades_contencion_empiricas <- function(resultados, theta_0, rho) {
  # 'resultados' debe tener: n, alpha_hat, beta_hat (una fila por réplica)
  if (!all(c("n", "alpha_hat", "beta_hat") %in% names(resultados))) {
    stop("resultados debe contener las columnas: n, alpha_hat, beta_hat")
  }
  contained <- with(resultados, mapply(
    function(a_hat, b_hat, n_i) {
      esta_en_C(c(a_hat, b_hat), theta_0, n = as.integer(n_i), rho = rho)
    },
    alpha_hat, beta_hat, n
  ))
  df_flag <- data.frame(n = resultados$n, dentro = contained)

  # Resumen por n: porcentaje, conteo y IC binomial
  sumarizado <- lapply(split(df_flag$dentro, df_flag$n), function(x) {
    m <- length(x); k <- sum(x)
    pct <- 100 * k / m
    ci <- binom.test(k, m)$conf.int * 100
    c(m = m, k = k, porcentaje = pct, ci_lo = ci[1], ci_hi = ci[2])
  })
  out <- do.call(rbind, sumarizado)
  out <- data.frame(n = as.integer(rownames(out)), out, row.names = NULL)
  out[order(out$n), ]
}

theta_0 <- c(alpha_0, beta_0)  # definidos previamente en el notebook
calcular_probabilidades_contencion_empiricas(resultados, theta_0 = theta_0, rho = rho)
```

Visualizamos ahora los EMV $(\hat\alpha,\hat\beta)$ por tamaño muestral, coloreando en azul las réplicas que cumplen la condición y en rojo las que no.

```{r}
# Paquete para la visualización
if (!requireNamespace("ggplot2", quietly = TRUE)) {
  stop("Necesitas el paquete 'ggplot2'. Instálalo con install.packages('ggplot2').")
}
library(ggplot2)

plot_estimadores_grid <- function(resultados, theta_0, rho, ncol = 3) {
  stopifnot(all(c("n", "alpha_hat", "beta_hat") %in% names(resultados)))

  dentro <- with(resultados, mapply(
    function(a_hat, b_hat, n_i) {
      esta_en_C(c(a_hat, b_hat), theta_0, n = as.integer(n_i), rho = rho)
    },
    alpha_hat, beta_hat, n
  ))

  df <- transform(resultados, dentro = dentro)
  df$n <- factor(df$n, levels = sort(unique(df$n)))

  p <- ggplot(df, aes(x = alpha_hat, y = beta_hat, color = dentro)) +
    geom_point(size = 1.4, alpha = 0.7) +
    scale_color_manual(values = c(`TRUE` = "blue", `FALSE` = "red"),
                       breaks = c(TRUE, FALSE),
                       labels = c("Cumple", "No cumple"),
                       name = NULL) +
    facet_wrap(~ n, ncol = ncol, scales = "free") +
    labs(x = expression(hat(alpha)),
         y = expression(hat(beta)),
         title = "Estimadores EMV por tamaño muestral") +
    theme_bw() +
    theme(legend.position = "bottom")

  print(p)
  invisible(p)
}

plot_estimadores_grid(resultados, theta_0 = theta_0, rho = rho)
```

En línea con la teoría, la cobertura empírica tiende al $95\%$ a medida que crece $n$ y las nubes de puntos se concentran alrededor del valor verdadero $(\alpha_0,\beta_0)=(2.5,\,4.6)$. Pues, note que, en los paneles de $n$ más grandes, la proporción de puntos rojos (no contenidos) se reduce visiblemente y la dispersión alrededor de $(\alpha_0,\beta_0)$ disminuye. Más allá, la tabla obtenida muestra una clara mejora de la cobertura con el tamaño muestral:

-   Para $n=100$ y $n=200$ hay **subcobertura** notable (91.5% y 92.6%); los IC binomiales **no** contienen el 95%.
-   Desde $n=500$ en adelante, el 95% **sí queda dentro** del intervalo de confianza de la cobertura $[92.57,\,95.57]$, $[92.68,\,95.65]$, $[93.24,\,96.09]$, por lo que la cobertura es **compatible con la nominal**.
-   En $n=1000$–$2000$ la cobertura media (94.3%–94.8%) está a \<1 punto porcentual del 95% y la diferencia es **insignificante frente al error Monte Carlo** (ancho del IC ≈ 3 p.p. con 1000 réplicas).

Consecuentemente, el conjunto elíptico asintótico alcanza una calibración adecuada **a partir de** $n \approx 500$; para tamaños más pequeños presenta subcobertura de \~2–3.5 p.p., esperable por el carácter asintótico del método y el uso de $\mathcal{I}_F(\hat\theta)$ como aproximación.
